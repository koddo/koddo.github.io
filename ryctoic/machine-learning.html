<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Machine learning</title>
  <meta name="description" content="https://www.reddit.com/r/math/comments/4ieenr/calculus_and_backpropagation/d2xqaj7">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://koddo.github.io/ryctoic/machine-learning">
  <link rel="alternate" type="application/rss+xml" title="koddo on code" href="https://koddo.github.io/feed.xml">
</head>

  <body>
    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">koddo on code</a>

    <nav class="site-nav">
      <!-- <a href="#" class="menu-icon"> -->
      <!--   <svg viewBox="0 0 18 15"> -->
      <!--     <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/> -->
      <!--     <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/> -->
      <!--     <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/> -->
      <!--   </svg> -->
      <!-- </a> -->

      <div class="trigger">
        <!--  -->
        <!--    -->
        <!--   <a class="page-link" href="/aaa">AAA</a> -->
        <!--    -->
        <!--  -->
        <!--    -->
        <!--   <a class="page-link" href="/about">About</a> -->
        <!--    -->
        <!--  -->
        <!--    -->
        <!--  -->
        <!--    -->
        <!--  -->
        <!--    -->
        <!--  -->
        <!--    -->
        <!--  -->
        <!--    -->
        <!--   <a class="page-link" href="/site-map">Site map</a> -->
        <!--    -->
        <!--  -->
        <!-- <a style="fill: #ccc;" class="icon icon--rss" href="/feed.xml"><img src="icon-rss.svg" width="16px" height="16px" /></a> -->

          <a class="rss-feed" href="/feed.xml"><span class="icon icon--rss"><svg xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:cc="http://creativecommons.org/ns#" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg" xmlns="http://www.w3.org/2000/svg" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" viewBox="0 -256 1792 1792" id="svg2989" version="1.1" inkscape:version="0.48.3.1 r9886" width="100%" height="100%" sodipodi:docname="rss_font_awesome.svg">
  <metadata id="metadata2999">
    <rdf:RDF>
      <cc:Work rdf:about="">
        <dc:format>image/svg+xml</dc:format>
        <dc:type rdf:resource="http://purl.org/dc/dcmitype/StillImage"/>
      </cc:Work>
    </rdf:RDF>
  </metadata>
  <defs id="defs2997"/>
  <sodipodi:namedview pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" objecttolerance="10" gridtolerance="10" guidetolerance="10" inkscape:pageopacity="0" inkscape:pageshadow="2" inkscape:window-width="640" inkscape:window-height="480" id="namedview2995" showgrid="false" inkscape:zoom="0.13169643" inkscape:cx="896" inkscape:cy="896" inkscape:window-x="0" inkscape:window-y="25" inkscape:window-maximized="0" inkscape:current-layer="svg2989"/>
  <g transform="matrix(1,0,0,-1,212.61017,1346.1695)" id="g2991">
    <path d="M 384,192 Q 384,112 328,56 272,0 192,0 112,0 56,56 0,112 0,192 q 0,80 56,136 56,56 136,56 80,0 136,-56 56,-56 56,-136 z M 896,69 Q 898,41 879,21 861,0 832,0 H 697 Q 672,0 654,16.5 636,33 634,58 612,287 449.5,449.5 287,612 58,634 33,636 16.5,654 0,672 0,697 v 135 q 0,29 21,47 17,17 43,17 h 5 Q 229,883 375,815.5 521,748 634,634 748,521 815.5,375 883,229 896,69 z m 512,-2 Q 1410,40 1390,20 1372,0 1344,0 H 1201 Q 1175,0 1156.5,17.5 1138,35 1137,60 1125,275 1036,468.5 947,662 804.5,804.5 662,947 468.5,1036 275,1125 60,1138 35,1139 17.5,1157.5 0,1176 0,1201 v 143 q 0,28 20,46 18,18 44,18 h 3 Q 329,1395 568.5,1288 808,1181 994,994 1181,808 1288,568.5 1395,329 1408,67 z" id="path2993" inkscape:connector-curvature="0"/>
  </g>
</svg>
</span></a>
      </div>
    </nav>

  </div>

</header>

    <div class="page-content">
      <div class="wrapper">
        
  

  
    
    

<article class="post">
    <!-- TODO: any html tag for go up a level? -->
    <a href="/ryctoic">Ryctoic/</a>

  <header class="post-header">
    <h1 class="post-title no-anchor">Machine learning</h1>
  </header>

  <div class="post-content">
    <p><a href="https://www.reddit.com/r/math/comments/4ieenr/calculus_and_backpropagation/d2xqaj7">https://www.reddit.com/r/math/comments/4ieenr/calculus_and_backpropagation/d2xqaj7</a></p>

<p><a href="http://devzum.com/2015/06/best-machine-learning-cheat-sheets/">http://devzum.com/2015/06/best-machine-learning-cheat-sheets/</a></p>

<p><a href="https://s3.amazonaws.com/MLMastery/MachineLearningAlgorithms.png">https://s3.amazonaws.com/MLMastery/MachineLearningAlgorithms.png</a></p>

<p><a href="https://github.com/hangtwenty/dive-into-machine-learning">https://github.com/hangtwenty/dive-into-machine-learning</a></p>

<h1 id="misc">misc</h1>

<p>examples of statistics using pandas: <a href="https://github.com/rouseguy/intro2stats/tree/master/notebooks">https://github.com/rouseguy/intro2stats/tree/master/notebooks</a></p>

<p>sort:<br />
<a href="https://codesachin.wordpress.com/2016/06/25/non-mathematical-feature-engineering-techniques-for-data-science/">https://codesachin.wordpress.com/2016/06/25/non-mathematical-feature-engineering-techniques-for-data-science/</a><br />
<a href="https://www.countbayesie.com/blog/2016/5/1/a-guide-to-bayesian-statistics">https://www.countbayesie.com/blog/2016/5/1/a-guide-to-bayesian-statistics</a></p>

<p>A Visual Introduction to Machine Learning — <a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-1/">http://www.r2d3.us/visual-intro-to-machine-learning-part-1/</a><br />
<a href="https://algobeans.com/">https://algobeans.com/</a></p>

<p><a href="https://ml.berkeley.edu/blog/2016/11/06/tutorial-1/">https://ml.berkeley.edu/blog/2016/11/06/tutorial-1/</a><br />
<a href="https://ml.berkeley.edu/blog/2016/12/24/tutorial-2/">https://ml.berkeley.edu/blog/2016/12/24/tutorial-2/</a></p>

<p>Computational Statistics in Python 0.1 — <a href="http://people.duke.edu/~ccc14/sta-663/index.html">http://people.duke.edu/~ccc14/sta-663/index.html</a></p>

<h1 id="kaggle">Kaggle</h1>

<p>start with <a href="https://www.quora.com/What-Kaggle-competitions-should-a-beginner-start-with-1/answer/William-Chen-6?srid=Ywn4">https://www.quora.com/What-Kaggle-competitions-should-a-beginner-start-with-1/answer/William-Chen-6?srid=Ywn4</a></p>

<p>Binary Classification: Titanic: Machine Learning from Disaster</p>

<p>Multi-Class Classification: Forest Cover Type Prediction</p>

<p>Regression with temporal component: Bike Sharing Demand</p>

<p>Binary Classification with text data: Random Acts of Pizza</p>

<p>Digit Recognizer</p>

<p>Amazon Employee Access Challenge</p>

<p>Sentiment Analysis on Movie Reviews</p>

<h1 id="xgboost">XGBoost</h1>

<p><a href="https://xgboost.readthedocs.io/en/latest/model.html">https://xgboost.readthedocs.io/en/latest/model.html</a></p>

<h1 id="decision-trees">decision trees</h1>

<p><a href="https://en.wikipedia.org/wiki/C4.5_algorithm">https://en.wikipedia.org/wiki/C4.5_algorithm</a> is number one in top ten ml algorithms<br />
<a href="http://scikit-learn.org/stable/modules/tree.html">http://scikit-learn.org/stable/modules/tree.html</a></p>

<p>tutorials<br />
<a href="http://thegrimmscientist.com/2014/10/23/tutorial-decision-trees/">http://thegrimmscientist.com/2014/10/23/tutorial-decision-trees/</a><br />
<a href="http://people.revoledu.com/kardi/tutorial/DecisionTree/index.html">http://people.revoledu.com/kardi/tutorial/DecisionTree/index.html</a></p>

<h2 id="algorithm">algorithm</h2>

<p>we start at the root node</p>

<ol>
  <li>find a split that that maximizes information gain</li>
  <li>do the split</li>
  <li>recur into two new nodes</li>
</ol>

<p>stopping rules:</p>

<ul>
  <li>max depth is reached</li>
  <li>leaf nodes are pure</li>
  <li>splitting doesn’t lead to an information gain</li>
</ul>

<h2 id="gini-vs-entropy">gini vs entropy</h2>

<blockquote>
  <ul>
    <li>Gini is intended for continuous attributes, and Entropy for attributes that occur in classes (e.g. colors)</li>
    <li>“Gini” will tend to find the largest class, and “entropy” tends to find groups of classes that make up ~50% of the data</li>
    <li>“Gini” to minimize misclassification</li>
  </ul>
</blockquote>

<p>from <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.57.9764&amp;rep=rep1&amp;type=pdf">Theoretical Comparison between the Gini Index and Information Gain Criteria, by Laura E. Raileanu and Kilian Stoffel</a>:</p>

<blockquote>
  <p>[…] we were able to analyze the frequency of agreement/disagremment of the Gini Index function and the Information Gain function. We found that they disagree only in 2%, which explains why most previously published empirical results concluded that it is not possible to decide which one of the two tests to prefer.</p>
</blockquote>

<div class="ryctoic-questions">
  <ul>
    <li>q: gini vs entropy criterias for decision trees</li>
  </ul>
</div>

<p>sources:<br />
<a href="https://www.garysieling.com/blog/sklearn-gini-vs-entropy-criteria">https://www.garysieling.com/blog/sklearn-gini-vs-entropy-criteria</a><br />
<a href="https://sebastianraschka.com/faq/docs/decisiontree-error-vs-entropy.html">https://sebastianraschka.com/faq/docs/decisiontree-error-vs-entropy.html</a><br />
<a href="https://sebastianraschka.com/faq/docs/decision-tree-binary.html">https://sebastianraschka.com/faq/docs/decision-tree-binary.html</a><br />
<a href="http://haohanw.blogspot.ru/2014/08/ml-decision-tree-rule-selection.html">http://haohanw.blogspot.ru/2014/08/ml-decision-tree-rule-selection.html</a><br />
<a href="https://www.quora.com/What-are-the-advantages-of-different-Decision-Trees-Algorithms">https://www.quora.com/What-are-the-advantages-of-different-Decision-Trees-Algorithms</a><br />
<a href="https://www.quora.com/Are-gini-index-entropy-or-classification-error-measures-causing-any-difference-on-Decision-Tree-classification">https://www.quora.com/Are-gini-index-entropy-or-classification-error-measures-causing-any-difference-on-Decision-Tree-classification</a></p>

<h2 id="entropy">entropy</h2>

<p>impurity of split</p>

<p><img src="images/20160821-1436-498cTU.screenshot.png" alt="impurity illustration" /></p>

<h2 id="missing-values">missing values</h2>

<h1 id="svm">svm</h1>

<blockquote>
  <p>Why someone would call an algorithm a machine? I have no clue, it was invented by a russian.</p>

  <p>— Sebastian Thrun, <cite><a href="https://www.youtube.com/watch?v=mzKPXz-Yhwk&amp;index=68&amp;list=PLAwxTw4SYaPkQXg8TkVdIvYv4HfLG7SiH">Intro to Machine Learning</a></cite></p>
  <h1 id="neural-networks">neural networks</h1>
</blockquote>

<p><a href="http://neuralnetworksanddeeplearning.com/">http://neuralnetworksanddeeplearning.com/</a><br />
<a href="http://www.asimovinstitute.org/neural-network-zoo/">http://www.asimovinstitute.org/neural-network-zoo/</a><br />
<a href="https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap">https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap</a></p>

<p><a href="https://www.reddit.com/r/InternetIsBeautiful/comments/4emh2i/play_with_a_neural_network_right_in_your_browser/">https://www.reddit.com/r/InternetIsBeautiful/comments/4emh2i/play_with_a_neural_network_right_in_your_browser/</a></p>

<p>A Few Useful Things to Know about Machine Learning <a href="https://gist.github.com/shagunsodhani/5c2cdfc269bf8aa50b72">https://gist.github.com/shagunsodhani/5c2cdfc269bf8aa50b72</a>, <a href="http://machinelearningmastery.com/useful-things-to-know-about-machine-learning/">http://machinelearningmastery.com/useful-things-to-know-about-machine-learning/</a></p>

<p>Методы оптимизации нейронных сетей — <a href="https://habrahabr.ru/post/318970/">https://habrahabr.ru/post/318970/</a></p>

<h2 id="universal-approximation-theorem">universal approximation theorem</h2>

<p><a href="http://neuralnetworksanddeeplearning.com/chap4.html">http://neuralnetworksanddeeplearning.com/chap4.html</a><br />
<a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">https://en.wikipedia.org/wiki/Universal_approximation_theorem</a><br />
<a href="http://stackoverflow.com/questions/25609347/can-neural-networks-approximate-any-function-given-enough-hidden-neurons">http://stackoverflow.com/questions/25609347/can-neural-networks-approximate-any-function-given-enough-hidden-neurons</a><br />
<a href="http://cstheory.stackexchange.com/questions/17545/universal-approximation-theorem-neural-networks">http://cstheory.stackexchange.com/questions/17545/universal-approximation-theorem-neural-networks</a><br />
Regarding your example of f(x) = x2, yes you can approximate it with a neural network within any finite range: [-1,1], [0, 1000], etc.<br />
But again, there is no neural network (or any other approximation structure) with a finite number of parameters that can approximate f(x) = x2 for all x in [-∞, +∞].</p>

<p><a href="https://theneural.wordpress.com/2013/01/07/universal-approximation-and-depth/">https://theneural.wordpress.com/2013/01/07/universal-approximation-and-depth/</a><br />
Hidden layer grows $O(e^d)$, where $d$ is dimensionality of input space.</p>

<p><a href="http://stackoverflow.com/questions/8160183/neural-nets-as-universal-approximators">http://stackoverflow.com/questions/8160183/neural-nets-as-universal-approximators</a><br />
the theorem only says that a function can be represented by a neural net. It does not say whether this representation can be learned or that it would be efficient. In fact, for a single-layer net approximating a highly varying function, the size grows exponentially with the function’s complexity.</p>

<p>Quite. It’s not hard to come up with models or families of functions which share this property.<br />
What matters is not only whether they can learn it but how much data they need to learn it to a given degree of accuracy. This is the kind of question addressed by nonparametric statistics and statistical learning theory.<br />
the Weierstrass approximation theorem states that every continuous function defined on a closed interval [a, b] can be uniformly approximated as closely as desired by a polynomial function.</p>

<p><code class="highlighter-rouge">max(0, x)</code> for activation function: <a href="http://stats.stackexchange.com/questions/141960/deep-neural-nets-relus-removing-non-linearity">http://stats.stackexchange.com/questions/141960/deep-neural-nets-relus-removing-non-linearity</a></p>

<p>TODO: does svm has this property?</p>

<h2 id="sigmoid-vs-tanh">sigmoid vs tanh</h2>

<p><a href="http://stats.stackexchange.com/questions/101560/tanh-activation-function-vs-sigmoid-activation-function">http://stats.stackexchange.com/questions/101560/tanh-activation-function-vs-sigmoid-activation-function</a><br />
<a href="http://stats.stackexchange.com/questions/142348/tanh-vs-sigmoid-in-neural-net">http://stats.stackexchange.com/questions/142348/tanh-vs-sigmoid-in-neural-net</a><br />
<a href="https://www.quora.com/In-machine-learning-algorithms-why-is-sigmoid-function-used-primarily-and-not-functions-like-tanh-x-In-tanhx-for-eg-seem-to-split-y-axis-evenly-and-flatten-out-fast-as-x-approaches-+-infinity-The-range-being-sigmoid-0-1-and-tanh-x-1-1">https://www.quora.com/In-machine-learning-algorithms-why-is-sigmoid-function-used-primarily-and-not-functions-like-tanh-x-In-tanhx-for-eg-seem-to-split-y-axis-evenly-and-flatten-out-fast-as-x-approaches-+-infinity-The-range-being-sigmoid-0-1-and-tanh-x-1-1</a></p>

<p>Sign(sum) can be useful as an example of non-linear activation function.</p>

<p><a href="https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b">https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b</a></p>

<h2 id="perceptron">perceptron</h2>

<p>A single perceptron can only solve linearly separable problems.<br />
XOR is not one of them.</p>

<p>TODO: my own illustration<br />
From <a href="http://natureofcode.com/book/chapter-10-neural-networks/">http://natureofcode.com/book/chapter-10-neural-networks/</a>:</p>

<p><img src="images/20161026-1344-763ctN.screenshot.png" alt="alt" /></p>

<p><img src="images/20161026-1345-763p3T.screenshot.png" alt="alt" /></p>


  </div>

  
</article>

      </div>
    </div>
    <footer class="site-footer">

  <div class="wrapper">

      <!-- <h2 class="footer-heading">koddo on code</h2> -->

      <div class="footer-col-wrapper">
          <div class="footer-col footer-col-1">
    <!--     <ul class="contact-list"> -->
    <!--       <li>koddo on code</li> -->
    <!--        -->
    <!--     </ul> -->
          </div>

      <div class="footer-col footer-col-2">
          <div style="text-align: center;">
          <ul class="social-media-list">
              
              <li>
                  <a href="https://github.com/koddo"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">koddo</span></a>

              </li>
              

              
              <li>
                  <a href="https://twitter.com/introstatic"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">introstatic</span></a>

              </li>
              

              
              
              <li>
              </li>
              
          </ul>
          </div>
      </div>

      <div class="footer-col footer-col-3">
          <!-- <p>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
</p> -->
          <!-- the icon - code review by Arthur Shlain from the Noun Project - https://thenounproject.com/term/source-code/101170/ -->
          <!-- <a title="page source" href="https://github.com/koddo/koddo.github.io/blob/jekyll-source/_ryctoic/machine-learning.md"><span class="icon icon--source-code"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" x="0px" y="0px" viewBox="0 0 100 100" enable-background="new 0 0 100 100" xml:space="preserve"><path d="M81.8,35.7c-1-1-2.6-1-3.5,0c-1,1-1,2.6,0,3.5L80,41l9,9l-9,9l-1.8,1.8c-1,1-1,2.6,0,3.5c0.5,0.5,1.1,0.7,1.8,0.7  s1.3-0.2,1.8-0.7l12.5-12.5c1-1,1-2.6,0-3.5L81.8,35.7z"/><path d="M21.8,39.3c1-1,1-2.6,0-3.5c-1-1-2.6-1-3.5,0L5.7,48.2c-1,1-1,2.6,0,3.5l12.5,12.5c0.5,0.5,1.1,0.7,1.8,0.7s1.3-0.2,1.8-0.7  c1-1,1-2.6,0-3.5L20,59l-9-9l9-9L21.8,39.3z"/><path d="M60.8,63.7c4.1-3.2,6.7-8.2,6.7-13.7c0-9.6-7.9-17.5-17.5-17.5S32.5,40.4,32.5,50S40.4,67.5,50,67.5c2.3,0,4.5-0.5,6.5-1.3  l10.1,17.5c0.5,0.8,1.3,1.3,2.2,1.3c0.4,0,0.9-0.1,1.2-0.3c1.2-0.7,1.6-2.2,0.9-3.4L60.8,63.7z M50,62.5c-6.9,0-12.5-5.6-12.5-12.5  S43.1,37.5,50,37.5S62.5,43.1,62.5,50S56.9,62.5,50,62.5z"/></svg></span></a> -->
          <a class="page-source-link" href="https://github.com/koddo/koddo.github.io/blob/jekyll-source/_ryctoic/machine-learning.md">page source</a>
      </div>
    </div>


  </div>

</footer>

    <script src="http://jekyll-livereload.jekyll.dev.dnsdock:35729/livereload.js"></script>
    
    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/3.1.1/anchor.min.js"></script>
    <script>anchors.add().remove(".no-anchor");</script>
    
  </body>
</html>
